[
  {
    "objectID": "Untitled0.html",
    "href": "Untitled0.html",
    "title": "Alex Tovar - Data Science Portfolio",
    "section": "",
    "text": "# Import Libraries -------------------------------------------------------------\nimport pandas as pd\nfrom plotnine import ggplot, aes, geom_bar, labs, theme_minimal, facet_wrap\n\nhello!!\n\n# Reading Data -----------------------------------------------------------------\nnetflix = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/netflix_titles.csv')\n\n\n# Movies -----------------------------------------------------------------------\nmovies = netflix[ netflix['type'] == 'Movie']\n# movies.head()\n\nratings = ['G', 'PG', 'PG-13', 'R', 'NC-17']\nmovies2 = movies[ movies['rating'].isin(ratings) ]\nmovies2['rating'].value_counts()\n\nggplot(movies2, aes(x='rating')) + \\\n    geom_bar(fill='skyblue') + \\\n    labs(title='Movie Ratings Distribution', x='Rating', y='Count') + \\\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n# Tv Shows ---------------------------------------------------------------------\ntvshows = netflix[ netflix['type'] == 'TV Show']\n# tvshows.head()\n\ntvratings = ['TV-Y', 'TV-Y7', 'TV-G']\nshows2 = tvshows[ tvshows['rating'].isin(tvratings) ]\nshows2['rating'].value_counts()\n\nggplot(shows2, aes(x='rating')) + \\\n    geom_bar(fill='skyblue') + \\\n    labs(title='Movie Ratings Distribution', x='Rating', y='Count') + \\\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Machine_Learning/project4.html",
    "href": "Machine_Learning/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 4"
    ]
  },
  {
    "objectID": "Machine_Learning/project1.html",
    "href": "Machine_Learning/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 1"
    ]
  },
  {
    "objectID": "Machine_Learning/project3.html",
    "href": "Machine_Learning/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 3"
    ]
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Alex Tovar’s CV",
    "section": "",
    "text": "Student, Programmer and Aspiring Data Scientist\n\n\nMy name is Alex Tovar, I’m currently challenging myself to pursue a Data Science degree by mastering tools, such as libraries, programming languages, advanced mathematics, and methods to research data-driven knowledge to make decisions. Also, emphasizing in Machine Learning and Applied Mathematics.\n\n\n\nBrigham Young University-Idaho 2024-(PROJECTED CLASS OF 2026) BA in Data Science (General Mathematics Cluster) Statistics Minor High School Diploma, Class of 2024 Huntingtown High School, Maryland,\nAdvanced Placement Student: (Passed 7 AP classes + 2 Self Studied) GPA 3.8+ Awarded Maryland Biliteracy Seal\n\n\n\n(Awarded by Graduation) Data science certificate Machine Learning Certificate\n\n\n\nBilingual (Spanish) Adaptable Team Collaboration Problem Solver Analytical Skills"
  },
  {
    "objectID": "resume.html#objective",
    "href": "resume.html#objective",
    "title": "Alex Tovar’s CV",
    "section": "",
    "text": "My name is Alex Tovar, I’m currently challenging myself to pursue a Data Science degree by mastering tools, such as libraries, programming languages, advanced mathematics, and methods to research data-driven knowledge to make decisions. Also, emphasizing in Machine Learning and Applied Mathematics."
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Alex Tovar’s CV",
    "section": "",
    "text": "Brigham Young University-Idaho 2024-(PROJECTED CLASS OF 2026) BA in Data Science (General Mathematics Cluster) Statistics Minor High School Diploma, Class of 2024 Huntingtown High School, Maryland,\nAdvanced Placement Student: (Passed 7 AP classes + 2 Self Studied) GPA 3.8+ Awarded Maryland Biliteracy Seal"
  },
  {
    "objectID": "resume.html#certifications",
    "href": "resume.html#certifications",
    "title": "Alex Tovar’s CV",
    "section": "",
    "text": "(Awarded by Graduation) Data science certificate Machine Learning Certificate"
  },
  {
    "objectID": "resume.html#attributes",
    "href": "resume.html#attributes",
    "title": "Alex Tovar’s CV",
    "section": "",
    "text": "Bilingual (Spanish) Adaptable Team Collaboration Problem Solver Analytical Skills"
  },
  {
    "objectID": "resume.html#experience",
    "href": "resume.html#experience",
    "title": "Alex Tovar’s CV",
    "section": "Experience",
    "text": "Experience\n\nProgrammed with Java – 2 years\nWhile taking advanced placement courses, I’ve collaborated with study groups to complete extra work outside of class within a personal development environment to develop additional skills with programming fundamentals, concepts, algorithms, and tools such as VSCODE.\n\n\nTropical Smoothie – 1 year\nJune (2023) – September (2024) #### McDonald’s – (½) year Summer (2023) & Summer (2024) Worked both jobs over both summers working up to 64-hour weeks collaborating, maintaining logistics, solving problems, and working in a fast-paced environment."
  },
  {
    "objectID": "resume.html#extracurriculars",
    "href": "resume.html#extracurriculars",
    "title": "Alex Tovar’s CV",
    "section": "Extracurriculars",
    "text": "Extracurriculars\n\nData Science Society Projects\nTo gain experience in a simulated and collaborative environment, contributing to the “Blood Test” project designed to provide incoming freshman experience and familiarity with data science practices and tools. Through this project, participants will become familiar with data visualization, grouping and aggregating data, discovering trends or patterns, and identifying outliers.\n\n\nIndependent Programmer (Python, R, SQL, Matplotlib,\nNumPy, Pandas, Spark) Independently contributing to projects and repositories on GitHub to build analytical and problem-solving skills outside of the classroom environment. This allows me to diversify in practical programming languages to collaborate on more projects in the future."
  },
  {
    "objectID": "MLPredictions.html",
    "href": "MLPredictions.html",
    "title": "Alex Tovar - Data Science Portfolio",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n\nwine_data = pd.read_csv('wine-training.csv')\n(wine_data).head()\n\n\n  \n    \n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod\nproline\nwine\n\n\n\n\n0\n13.71\n1.86\n2.36\n16.6\n101\n2.61\n2.88\n0.27\n1.69\n3.80\n1.11\n4.00\n1035\n0\n\n\n1\n13.88\n5.04\n2.23\n20.0\n80\n0.98\n0.34\n0.40\n0.68\n4.90\n0.58\n1.33\n415\n2\n\n\n2\n12.29\n1.41\n1.98\n16.0\n85\n2.55\n2.50\n0.29\n1.77\n2.90\n1.23\n2.74\n428\n1\n\n\n3\n12.21\n1.19\n1.75\n16.8\n151\n1.85\n1.28\n0.14\n2.50\n2.85\n1.28\n3.07\n718\n1\n\n\n4\n12.82\n3.37\n2.30\n19.5\n88\n1.48\n0.66\n0.40\n0.97\n10.26\n0.72\n1.75\n685\n2\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# Prepare the data\nX = wine_data.drop('wine', axis=1)  # Features (all columns except 'wine')\ny = wine_data['wine']  # Target variable ('wine' column)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a model\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# Make predictions\npredictions = model.predict(X_test)\n\n# Evaluate the model (optional)\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy: {accuracy}\")\n\nAccuracy: 0.9166666666666666\n\n\n\n# Save predictions to CSV\nsubmission_df = pd.DataFrame(predictions, columns=['Class'])\nsubmission_df.to_csv('wine_predictions.csv', index=False)\n\nHoldout\n\nwine_holdout = pd.read_csv('wine-holdout.csv')\n\n\n# prompt: make predictions of the wine training dataset against the holdout\n\n# Make predictions on the holdout set\nholdout_predictions = model.predict(wine_holdout)\n\n# Create a submission DataFrame for the holdout predictions\nholdout_submission_df = pd.DataFrame(holdout_predictions, columns=['Class'])\n\n# Save the holdout predictions to a CSV file\nholdout_submission_df.to_csv('wine_holdout_predictions.csv', index=False)\n\nAccuracy on test set: 0.9166666666666666\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Assuming 'model' and 'X' are already defined from your previous code\n\n# Get feature importances\nimportances = model.feature_importances_\nfeature_names = X.columns\n\n# Create a Pandas Series\nfeat_importances = pd.Series(importances, index=feature_names)\n\n# Plot the horizontal bar chart\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.title('Top 10 Feature Importances')\nplt.xlabel('Importance Score')\nplt.ylabel('Feature')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import recall_score\n\n\n# Load the training data\nwine_data = pd.read_csv('wine-training.csv')\n\n# Separate features (X) and target (y)\nX = wine_data.drop('wine', axis=1)\ny = wine_data['wine']\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Identify numerical and categorical features (if any)\nnumerical_features = X.select_dtypes(include=['number']).columns\ncategorical_features = X.select_dtypes(include=['object']).columns\n\n# Create transformers for numerical and categorical features\nnumerical_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())  # Normalization\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encoding\n])\n\n# Combine transformers using ColumnTransformer\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_features),\n        ('cat', categorical_transformer, categorical_features),\n    ])\n\n# Create a pipeline with preprocessing and model\nmodel = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced'))  # Aim for recall\n])\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions on the holdout set\nwine_holdout = pd.read_csv('wine-holdout.csv')\nholdout_predictions = model.predict(wine_holdout)\n\n# Create a submission DataFrame\nholdout_submission_df = pd.DataFrame(holdout_predictions, columns=['Class'])\n\n# Save the predictions to a CSV file\nholdout_submission_df.to_csv('wine_holdout_predictions.csv', index=False)\n\n# Evaluate on test set (optional)\npredictions = model.predict(X_test)\nrecall = recall_score(y_test, predictions, average='weighted')  # Calculate recall\nprint(f\"Recall: {recall}\")\n\nRecall: 0.9166666666666666\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n\n# ... (Previous code for model training and prediction) ...\n\n# Evaluate on the test set\ntest_predictions = model.predict(X_test)\n\n# Calculate metrics for the test set\ntest_accuracy = accuracy_score(y_test, test_predictions)\ntest_recall = recall_score(y_test, test_predictions, average='weighted')\ntest_precision = precision_score(y_test, test_predictions, average='weighted')\ntest_f1 = f1_score(y_test, test_predictions, average='weighted')\ntest_confusion_matrix = confusion_matrix(y_test, test_predictions)\n\n# Print test set metrics\nprint(\"Test Set Metrics:\")\nprint(f\"Accuracy: {test_accuracy}\")\nprint(f\"Recall: {test_recall}\")\nprint(f\"Precision: {test_precision}\")\nprint(f\"F1-Score: {test_f1}\")\nprint(\"Confusion Matrix:\")\nprint(test_confusion_matrix)\n\n# Evaluate on the holdout set\nholdout_predictions = model.predict(wine_holdout)  # Assuming 'wine_holdout' is your holdout data\n\n# Since true labels for holdout are usually unknown, we can't calculate metrics\n# Instead, focus on saving predictions for submission\n\n# Create a submission DataFrame\nholdout_submission_df = pd.DataFrame(holdout_predictions, columns=['Class'])\n\n# Save the predictions to a CSV file\nholdout_submission_df.to_csv('wine_holdout_predictions.csv', index=False)\n\nprint(\"\\nHoldout predictions saved to 'wine_holdout_predictions.csv'\")\n\nTest Set Metrics:\nAccuracy: 0.9166666666666666\nRecall: 0.9166666666666666\nPrecision: 0.921875\nF1-Score: 0.9166666666666666\nConfusion Matrix:\n[[7 1 0]\n [0 8 1]\n [0 0 7]]\n\nHoldout predictions saved to 'wine_holdout_predictions.csv'\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Story_Telling/project4.html",
    "href": "Story_Telling/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 4"
    ]
  },
  {
    "objectID": "Story_Telling/project1.html",
    "href": "Story_Telling/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 1"
    ]
  },
  {
    "objectID": "Story_Telling/project3.html",
    "href": "Story_Telling/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 3"
    ]
  },
  {
    "objectID": "Competition/project2.html",
    "href": "Competition/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 2"
    ]
  },
  {
    "objectID": "Competition/project5.html",
    "href": "Competition/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 5"
    ]
  },
  {
    "objectID": "Module4.html",
    "href": "Module4.html",
    "title": "Alex Tovar - Data Science Portfolio",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Load the training dataset\ndata_url = \"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes.csv\"\ndata = pd.read_csv(data_url)\n\n\n# Load the mini holdout dataset\nmini_holdout_url = \"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/biking_holdout_test_mini.csv\"\nmini_holdout = pd.read_csv(mini_holdout_url)\n\n\n# Add the 'count' column to the training dataset\ndata['count'] = data['casual'] + data['registered']\n\n\n# Drop the 'casual' and 'registered' columns\ndata = data.drop(columns=['casual', 'registered'])\n\n# Convert 'dteday' to datetime format\ndata['dteday'] = pd.to_datetime(data['dteday'])\nmini_holdout['dteday'] = pd.to_datetime(mini_holdout['dteday'])\n\n# Function to engineer features\ndef engineer_features(df):\n    df = df.copy()\n\n    # Extract temporal features\n    df['year'] = df['dteday'].dt.year\n    df['month'] = df['dteday'].dt.month\n    df['day'] = df['dteday'].dt.day\n    df['day_of_week'] = df['dteday'].dt.dayofweek\n    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n\n    # Cyclical encoding for hour (captures time of day patterns)\n    df['hour_sin'] = np.sin(2 * np.pi * df['hr'] / 24)\n    df['hour_cos'] = np.cos(2 * np.pi * df['hr'] / 24)\n\n    # Cyclical encoding for month (captures seasonal patterns)\n    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n\n    # Cyclical encoding for day of week (captures weekly patterns)\n    df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n    df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n\n    # Interaction features (capture combined effects)\n    df['temp_hum'] = df['temp_c'] * df['hum']  # Hot and humid days impact differently\n    df['temp_windspeed'] = df['temp_c'] * df['windspeed']  # Wind chill effect\n    df['hum_windspeed'] = df['hum'] * df['windspeed']  # Combined weather impact\n    # Enhanced holiday/workingday features\n    df['special_day'] = ((df['holiday'] == 1) | (df['is_weekend'] == 1)).astype(int)\n\n    # Create hour groups with unique labels\n    df['hour_group'] = pd.cut(df['hr'],\n                              bins=[-1, 5, 11, 16, 20, 24],\n                              labels=['early_morning', 'morning', 'afternoon', 'evening', 'night'])\n\n    # Create \"rush hour\" feature (typical commuting times)\n    df['rush_hour'] = ((df['hr'] &gt;= 7) & (df['hr'] &lt;= 9) |\n                       (df['hr'] &gt;= 16) & (df['hr'] &lt;= 18)).astype(int)\n\n    # Weather and holiday interaction (bad weather on holidays has different impact)\n    df['weather_holiday'] = df['weathersit'] * df['holiday']\n\n    # COVID-19 periods (since dataset spans 2011-2023)\n    df['pre_covid'] = (df['dteday'] &lt; '2020-03-01').astype(int)\n    df['early_covid'] = ((df['dteday'] &gt;= '2020-03-01') & (df['dteday'] &lt; '2021-06-01')).astype(int)\n    df['late_covid'] = ((df['dteday'] &gt;= '2021-06-01') & (df['dteday'] &lt; '2022-04-01')).astype(int)\n    df['post_covid'] = (df['dteday'] &gt;= '2022-04-01').astype(int)\n\n    # Long-term trend capture (years since start of data)\n    df['years_since_start'] = (df['dteday'].dt.year - 2011) + (df['dteday'].dt.month - 1)/12\n\n    # Interaction between COVID periods and other features\n    df['covid_weather'] = ((df['early_covid'] == 1) | (df['late_covid'] == 1)) * df['weathersit']\n    df['covid_weekend'] = ((df['early_covid'] == 1) | (df['late_covid'] == 1)) * df['is_weekend']\n\n    # Create one-hot encoding for hour_group\n    hour_group_dummies = pd.get_dummies(df['hour_group'], prefix='hour_group')\n    df = pd.concat([df.drop(columns=['hour_group']), hour_group_dummies], axis=1)\n\n    # Drop 'dteday' after extracting all features\n    df = df.drop(columns=['dteday'])\n\n    return df\n\n\nplt.figure(figsize=(12, 6))\nax = sns.boxplot(x=data['season'], y=data['count'])  # Assign the plot to 'ax'\nplt.title('Bike Rentals by Season')\nplt.ylabel('Number of Rentals')\n\n   # Set custom x-axis tick labels\nax.set_xticklabels(['Winter', 'Spring', 'Summer', 'Fall'])\n\nplt.show()\n\nUserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(['Winter', 'Spring', 'Summer', 'Fall'])\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.ticker as ticker\n\n# Apply feature engineering\ndata = engineer_features(data)\nmini_holdout = engineer_features(mini_holdout)\n\nsns.boxplot(x=data['hr'], y=data['count'])\n\n# Visualize the relationship between hour and count\nplt.figure(figsize=(12, 6))\nplt.xticks(range(24), range(24))\nsns.boxplot(x=data['hr'], y=data['count'])\nplt.title('Bike Rentals by Hour')\nplt.xlabel('Hour of Day')\nplt.ylabel('Number of Rentals')\nplt.gca().xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\nplt.show()\n\n# Visualize the relationship between season and count\nplt.figure(figsize=(12, 6))\nsns.boxplot(x=data['season'], y=data['count'])\nplt.title('Bike Rentals by Season')\nplt.xlabel('Season (1:winter, 2:spring, 3:summer, 4:fall)')\nplt.ylabel('Number of Rentals')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata.head()\n\n\n  \n    \n\n\n\n\n\n\ndteday\nhr\ncasual\nregistered\ntemp_c\nfeels_like_c\nhum\nwindspeed\nweathersit\nseason\nholiday\nworkingday\n\n\n\n\n0\n1/1/2011\n0.0\n3\n13\n3.0\n3.0\n0.7957\n0.8\n1\n1\n0\n0\n\n\n1\n1/1/2011\n1.0\n8\n30\n1.7\n1.7\n0.8272\n0.8\n1\n1\n0\n0\n\n\n2\n1/1/2011\n2.0\n5\n26\n1.9\n1.9\n0.8157\n1.1\n1\n1\n0\n0\n\n\n3\n1/1/2011\n3.0\n3\n9\n2.5\n2.5\n0.7831\n0.8\n1\n1\n0\n0\n\n\n4\n1/1/2011\n4.0\n0\n1\n2.0\n2.0\n0.8075\n1.1\n1\n1\n0\n0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# Identify categorical features\ncategorical_features = ['season', 'weathersit', 'holiday', 'workingday']\n\n# One-hot encode categorical features\nencoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\nencoded_features = encoder.fit_transform(data[categorical_features])\nencoded_columns = encoder.get_feature_names_out(categorical_features)\nencoded_df = pd.DataFrame(encoded_features, columns=encoded_columns)\ndata = pd.concat([data.drop(columns=categorical_features), encoded_df], axis=1)\n\n# Identify numerical features (exclude 'count' and any other non-feature columns)\nnumerical_features = [col for col in data.columns\n                      if col not in ['count', 'instant']\n                      and not col.startswith('season_')\n                      and not col.startswith('weathersit_')\n                      and not col.startswith('holiday_')\n                      and not col.startswith('workingday_')]\n\n# Normalize numerical features\nscaler = StandardScaler()\ndata[numerical_features] = scaler.fit_transform(data[numerical_features])\n\n# Define features (X) and target (y)\nX = data.drop(columns=['count'])\ny = data['count']\n\n# Split into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(\"Training a gradient boosting model first to determine feature importance...\")\n# Train a gradient boosting model to identify important features\ngb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\ngb_model.fit(X_train, y_train)\n\nfeature_label_mapping = {\n    'dteday' : 'date',\n 'season' : 'season (1:winter, 2:spring, 3:summer, 4:fall)',\n 'hr' : 'hour (0 to 23)',\n'casual' : 'weather day is holiday or not',\n 'registered' : 'if day is neither weekend nor holiday is 1, otherwise is 0.',\n 'temp_c' : 'lol',\n 'feels_like_c': 'Clear, Few clouds, Partly cloudy, Partly cloudy',\n 'hum': 'Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist',\n 'windspeed': 'Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds',\n 'weathersit': 'Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog',\n 'season' : 'temperature in Celsius.',\n 'holiday': 'Feels like temperature in Celsius.',\n 'workingday': 'humidity percentage'\n}\n\n# Get feature importances\nfeature_importances = pd.DataFrame({\n    'Feature': X_train.columns,\n    'Importance': gb_model.feature_importances_\n}).sort_values('Importance', ascending=False).head(15)\n\nfeature_importances['Feature'] = feature_importances['Feature'].map(feature_label_mapping).fillna(feature_importances['Feature'])\n\nprint(\"Top 15 most important features:\")\nprint(feature_importances.head(15))\n\n# Visualize feature importances\nplt.figure(figsize=(12, 8))\nsns.barplot(x='Importance', y='Feature', data=feature_importances.head(15))\nplt.title('Top 15 Feature Importances')\nplt.tight_layout()\nplt.show()\n\nTraining a gradient boosting model first to determine feature importance...\nTop 15 most important features:\n                                              Feature  Importance\n0                                      hour (0 to 23)    0.271983\n11                                           hour_cos    0.117883\n26                                  years_since_start    0.097626\n10                                           hour_sin    0.095762\n2     Clear, Few clouds, Partly cloudy, Partly cloudy    0.088118\n1                                                 lol    0.068890\n20                                          rush_hour    0.059843\n44                                       workingday_0    0.022848\n3   Mist + Cloudy, Mist + Broken clouds, Mist + Fe...    0.021903\n45                                       workingday_1    0.018684\n27                                      covid_weather    0.014426\n31                               hour_group_afternoon    0.014409\n33                                   hour_group_night    0.013201\n19                                        special_day    0.012923\n32                                 hour_group_evening    0.012375\n\n\n\n\n\n\n\n\n\n\nprint(\"\\nNow training the neural network model...\")\n# Build the neural network with attention to the most important features\nmodel = Sequential([\n    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n    Dropout(0.3),\n    Dense(128, activation='relu'),\n    Dropout(0.3),\n    Dense(64, activation='relu'),\n    Dropout(0.3),\n    Dense(32, activation='relu'),\n    Dropout(0.2),\n    Dense(1, activation='relu')  # ReLU activation ensures non-negative predictions\n])\n\n# Compile the model with a better optimizer\noptimizer = Adam(learning_rate=0.001)\nmodel.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n\n# Callbacks for early stopping and learning rate scheduling\nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=15,\n    restore_best_weights=True,\n    verbose=1\n)\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.2,\n    patience=5,\n    min_lr=0.0001,\n    verbose=1\n)\n# Train the model with callbacks\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=50,  # Increased epochs but using early stopping\n    batch_size=32,\n    callbacks=[early_stopping, reduce_lr],\n    verbose=1\n)\n\n# Process the holdout dataset\n# One-hot encode categorical features\nencoded_features_holdout = encoder.transform(mini_holdout[categorical_features])\nencoded_df_holdout = pd.DataFrame(encoded_features_holdout, columns=encoded_columns)\nmini_holdout = pd.concat([mini_holdout.drop(columns=categorical_features), encoded_df_holdout], axis=1)\n\n# Ensure mini_holdout has the same columns as X_train\nfor col in X_train.columns:\n    if col not in mini_holdout.columns:\n        mini_holdout[col] = 0  # Add missing columns with default value\n\n# Reorder columns to match X_train\nmini_holdout = mini_holdout[X_train.columns]\n\n# Normalize numerical features\nmini_holdout[numerical_features] = scaler.transform(mini_holdout[numerical_features])\n\n\nNow training the neural network model...\nEpoch 1/50\n\n\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n1008/2812 ━━━━━━━━━━━━━━━━━━━━ 8s 5ms/step - loss: 67458.7422 - mae: 164.4174\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\n&lt;ipython-input-108-8b778d5e1175&gt; in &lt;cell line: 0&gt;()\n     33 )\n     34 # Train the model with callbacks\n---&gt; 35 history = model.fit(\n     36     X_train, y_train,\n     37     validation_data=(X_val, y_val),\n\n/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py in error_handler(*args, **kwargs)\n    115         filtered_tb = None\n    116         try:\n--&gt; 117             return fn(*args, **kwargs)\n    118         except Exception as e:\n    119             filtered_tb = _process_traceback_frames(e.__traceback__)\n\n/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\n    369                 for step, iterator in epoch_iterator:\n    370                     callbacks.on_train_batch_begin(step)\n--&gt; 371                     logs = self.train_function(iterator)\n    372                     callbacks.on_train_batch_end(step, logs)\n    373                     if self.stop_training:\n\n/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py in function(iterator)\n    217                 iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n    218             ):\n--&gt; 219                 opt_outputs = multi_step_on_iterator(iterator)\n    220                 if not opt_outputs.has_value():\n    221                     raise StopIteration\n\n/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs)\n    148     filtered_tb = None\n    149     try:\n--&gt; 150       return fn(*args, **kwargs)\n    151     except Exception as e:\n    152       filtered_tb = _process_traceback_frames(e.__traceback__)\n\n/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py in __call__(self, *args, **kwds)\n    831 \n    832       with OptionalXlaContext(self._jit_compile):\n--&gt; 833         result = self._call(*args, **kwds)\n    834 \n    835       new_tracing_count = self.experimental_get_tracing_count()\n\n/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py in _call(self, *args, **kwds)\n    876       # In this case we have not created variables on the first call. So we can\n    877       # run the first trace but we should fail if variables are created.\n--&gt; 878       results = tracing_compilation.call_function(\n    879           args, kwds, self._variable_creation_config\n    880       )\n\n/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py in call_function(args, kwargs, tracing_options)\n    137   bound_args = function.function_type.bind(*args, **kwargs)\n    138   flat_inputs = function.function_type.unpack_inputs(bound_args)\n--&gt; 139   return function._call_flat(  # pylint: disable=protected-access\n    140       flat_inputs, captured_inputs=function.captured_inputs\n    141   )\n\n/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py in _call_flat(self, tensor_inputs, captured_inputs)\n   1320         and executing_eagerly):\n   1321       # No tape is watching; skip to running the function.\n-&gt; 1322       return self._inference_function.call_preflattened(args)\n   1323     forward_backward = self._select_forward_and_backward_functions(\n   1324         args,\n\n/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py in call_preflattened(self, args)\n    214   def call_preflattened(self, args: Sequence[core.Tensor]) -&gt; Any:\n    215     \"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\n--&gt; 216     flat_outputs = self.call_flat(*args)\n    217     return self.function_type.pack_output(flat_outputs)\n    218 \n\n/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py in call_flat(self, *args)\n    249         with record.stop_recording():\n    250           if self._bound_context.executing_eagerly():\n--&gt; 251             outputs = self._bound_context.call_function(\n    252                 self.name,\n    253                 list(args),\n\n/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py in call_function(self, name, tensor_inputs, num_outputs)\n   1681     cancellation_context = cancellation.context()\n   1682     if cancellation_context is None:\n-&gt; 1683       outputs = execute.execute(\n   1684           name.decode(\"utf-8\"),\n   1685           num_outputs=num_outputs,\n\n/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\n     51   try:\n     52     ctx.ensure_initialized()\n---&gt; 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n     54                                         inputs, attrs, num_outputs)\n     55   except core._NotOkStatusException as e:\n\nKeyboardInterrupt: \n\n\n\n\n# Generate predictions\npredicted_counts = model.predict(mini_holdout).flatten()\n\n# Post-processing to ensure predictions are positive integers\npredicted_counts = np.maximum(0, predicted_counts)  # Ensure non-negative\npredicted_counts = predicted_counts.round(0).astype(int)\n\n# Save predictions to a CSV file\nresults = pd.DataFrame({'count': predicted_counts})\nresults.to_csv(\"team8-bike-rental-predictions.csv\", index=False)\n\n# Plot training history\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['mae'], label='Training MAE')\nplt.plot(history.history['val_mae'], label='Validation MAE')\nplt.title('Model MAE')\nplt.xlabel('Epochs')\nplt.ylabel('MAE')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-62-6b0f37726ebe&gt; in &lt;cell line: 0&gt;()\n      1 # Generate predictions\n----&gt; 2 predicted_counts = model.predict(mini_holdout).flatten()\n      3 \n      4 # Post-processing to ensure predictions are positive integers\n      5 predicted_counts = np.maximum(0, predicted_counts)  # Ensure non-negative\n\nNameError: name 'model' is not defined\n\n\n\n\n# Output model summary and evaluation metrics\nprint(\"\\nModel Summary:\")\nmodel.summary()\n\nprint(\"\\nTraining completed with early stopping at epoch:\",\n      len(history.history['loss']))\n\nprint(\"\\nValidation Loss:\", history.history['val_loss'][-1])\nprint(\"Validation MAE:\", history.history['val_mae'][-1])\n\nprint(\"\\nChecking for negative or extreme predictions:\")\nprint(\"Min prediction:\", min(predicted_counts))\nprint(\"Max prediction:\", max(predicted_counts))\nprint(\"Average prediction:\", sum(predicted_counts)/len(predicted_counts))\n\n# Additional insights\nprint(\"\\nDistribution of predictions:\")\nbins = [0, 50, 100, 200, 500, 1000, float('inf')]\nbin_labels = ['0-50', '51-100', '101-200', '201-500', '501-1000', '1000+']\nprediction_distribution = pd.cut(predicted_counts, bins=bins, labels=bin_labels)\nprint(pd.value_counts(prediction_distribution, normalize=True).sort_index() * 100)\n\n# Check if we have good variety in our predictions (not just the same value repeated)\nunique_predictions = len(set(predicted_counts))\nprint(f\"\\nNumber of unique prediction values: {unique_predictions} out of {len(predicted_counts)} predictions\")\nprint(f\"Percentage of unique values: {unique_predictions/len(predicted_counts)*100:.2f}%\")\n\n\nModel Summary:\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-1-bbd17864b0a6&gt; in &lt;cell line: 0&gt;()\n      1 # Output model summary and evaluation metrics\n      2 print(\"\\nModel Summary:\")\n----&gt; 3 model.summary()\n      4 \n      5 print(\"\\nTraining completed with early stopping at epoch:\",\n\nNameError: name 'model' is not defined\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Projects/project2.html",
    "href": "Cleansing_Projects/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 2"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project5.html",
    "href": "Cleansing_Projects/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 5"
    ]
  },
  {
    "objectID": "Cleansing_Exploration/project4.html",
    "href": "Cleansing_Exploration/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project1.html",
    "href": "Cleansing_Exploration/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project3.html",
    "href": "Cleansing_Exploration/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html#title-2-header",
    "href": "index.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Full_Stack/project2.html",
    "href": "Full_Stack/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 2"
    ]
  },
  {
    "objectID": "Full_Stack/project5.html",
    "href": "Full_Stack/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 5"
    ]
  },
  {
    "objectID": "notebooks/starter_housing.html",
    "href": "notebooks/starter_housing.html",
    "title": "Alex Tovar - Data Science Portfolio",
    "section": "",
    "text": "!pip install scikit-learn==1.5.2\n\nRequirement already satisfied: scikit-learn==1.5.2 in /usr/local/lib/python3.11/dist-packages (1.5.2)\nRequirement already satisfied: numpy&gt;=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.26.4)\nRequirement already satisfied: scipy&gt;=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.13.1)\nRequirement already satisfied: joblib&gt;=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.4.2)\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (3.5.0)\n\n\n\nimport pandas as pd\n\nhousing = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/housing.csv')\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport xgboost as xgb\n\n# Load dataset (replace with actual filename)\ndata = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/housing.csv')\n# Drop the 'id' column as it is not a useful feature\ndata = data.drop(columns=['id'])\n\n# Extract 'year_sold' from the 'date' column and drop the original 'date' column\ndata['year_sold'] = data['date'].str[:4].astype(int)\ndata = data.drop(columns=['date'])\n\n# Select only the 7 most relevant features\nselected_features = ['sqft_living', 'bedrooms', 'bathrooms', 'grade', 'condition', 'yr_built', 'zipcode']\n\nX = data.drop(columns=['price','zipcode'])\n\n# X = data[selected_features]  # Features\ny = data['price']  # Target\n\n# Encode 'zipcode' as it is categorical\n# if 'zipcode' in X.columns:\n#     le = LabelEncoder()\n#     X.loc[:, 'zipcode'] = LabelEncoder().fit_transform(X['zipcode'])\n\n# Split the dataset into training (80%) and test (20%) sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train the XGBoost model\nxgb_model = xgb.XGBRegressor(\n    objective='reg:squarederror',\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=42\n)\n\nxgb_model.fit(X_train, y_train)\n\n# Compute R^2 scores for training and test sets\ntrain_score = xgb_model.score(X_train, y_train)\ntest_score = xgb_model.score(X_test, y_test)\n\nprint(f\"Train Score: {train_score:.4f}\")\nprint(f\"Test Score: {test_score:.4f}\")\n\nTrain Score: 0.9761\nTest Score: 0.8800\n\n\n\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load new dataset (for prediction)\nnew_data_path = \"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/housing_holdout_test_mini.csv\"\nnew_data = pd.read_csv(new_data_path)\n\n# Extract 'year_sold' from 'date' and drop 'id' & 'date'\nnew_data['year_sold'] = new_data['date'].str[:4].astype(int)\nnew_data = new_data.drop(columns=['id', 'date'])\n\n# Select all features dynamically (excluding 'price' if it exists)\nX_new = new_data.drop(columns=['price','zipcode'], errors='ignore').copy()  # Exclude 'price' only if present\n # Ideally, reuse the trained encoder\n\n# Predict prices using the trained model\npredicted_prices = xgb_model.predict(X_new)\n\n# Round predictions to the nearest whole number\npredicted_prices = predicted_prices.round(0)\n\n# Save predictions as a single-column CSV with header 'price'\nresults = pd.DataFrame({'price': predicted_prices})\nresults.to_csv(\"team8-module3-predictions.csv\", index=False)\n\nprint(\"Predictions saved to team8-module3-predictions.csv\")\n\nPredictions saved to team8-module3-predictions.csv\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cleansing.html",
    "href": "cleansing.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Cleansing"
    ]
  },
  {
    "objectID": "cleansing.html#title-2-header",
    "href": "cleansing.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Cleansing"
    ]
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Machine Learning"
    ]
  },
  {
    "objectID": "ml.html#title-2-header",
    "href": "ml.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Machine Learning"
    ]
  },
  {
    "objectID": "Module4Training.html",
    "href": "Module4Training.html",
    "title": "Alex Tovar - Data Science Portfolio",
    "section": "",
    "text": "# prompt: use this link for the data: https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes.csv\n\nimport pandas as pd\n\n# Load the dataframe.\ndf_bikes = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes.csv')\n\n# Print some info.\ndf_bikes.info()\nprint(df_bikes.head())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 112475 entries, 0 to 112474\nData columns (total 12 columns):\n #   Column        Non-Null Count   Dtype  \n---  ------        --------------   -----  \n 0   dteday        112475 non-null  object \n 1   hr            112475 non-null  float64\n 2   casual        112475 non-null  int64  \n 3   registered    112475 non-null  int64  \n 4   temp_c        112475 non-null  float64\n 5   feels_like_c  112475 non-null  float64\n 6   hum           112475 non-null  float64\n 7   windspeed     112475 non-null  float64\n 8   weathersit    112475 non-null  int64  \n 9   season        112475 non-null  int64  \n 10  holiday       112475 non-null  int64  \n 11  workingday    112475 non-null  int64  \ndtypes: float64(5), int64(6), object(1)\nmemory usage: 10.3+ MB\n     dteday   hr  casual  registered  temp_c  feels_like_c     hum  windspeed  \\\n0  1/1/2011  0.0       3          13     3.0           3.0  0.7957        0.8   \n1  1/1/2011  1.0       8          30     1.7           1.7  0.8272        0.8   \n2  1/1/2011  2.0       5          26     1.9           1.9  0.8157        1.1   \n3  1/1/2011  3.0       3           9     2.5           2.5  0.7831        0.8   \n4  1/1/2011  4.0       0           1     2.0           2.0  0.8075        1.1   \n\n   weathersit  season  holiday  workingday  \n0           1       1        0           0  \n1           1       1        0           0  \n2           1       1        0           0  \n3           1       1        0           0  \n4           1       1        0           0  \n\n\n\n# prompt: make a tensorflow neural network model\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\n\n# Assuming 'count' is the target variable and other columns are features\nX = df_bikes.drop('count', axis=1)\ny = df_bikes['count']\n\n# Convert categorical features to numerical using one-hot encoding if needed\nX = pd.get_dummies(X, columns=['season', 'yr', 'mnth', 'holiday', 'weekday', 'workingday', 'weathersit'])\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the model\nmodel = keras.Sequential([\n    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(1) # Output layer for regression\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mse', metrics=['mae']) # Use appropriate loss and metrics for regression\n\n# Train the model\nmodel.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2) # Adjust epochs and batch size as needed\n\n# Evaluate the model\nloss, mae = model.evaluate(X_test, y_test)\nprint(f\"Mean Absolute Error: {mae}\")\n\n# Make predictions\npredictions = model.predict(X_test)\npredictions\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n&lt;ipython-input-4-e5af95e17599&gt; in &lt;cell line: 0&gt;()\n      6 \n      7 # Assuming 'count' is the target variable and other columns are features\n----&gt; 8 X = df_bikes.drop('count', axis=1)\n      9 y = df_bikes['count']\n     10 \n\n/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py in drop(self, labels, axis, index, columns, level, inplace, errors)\n   5579                 weight  1.0     0.8\n   5580         \"\"\"\n-&gt; 5581         return super().drop(\n   5582             labels=labels,\n   5583             axis=axis,\n\n/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py in drop(self, labels, axis, index, columns, level, inplace, errors)\n   4786         for axis, labels in axes.items():\n   4787             if labels is not None:\n-&gt; 4788                 obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n   4789 \n   4790         if inplace:\n\n/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py in _drop_axis(self, labels, axis, level, errors, only_slice)\n   4828                 new_axis = axis.drop(labels, level=level, errors=errors)\n   4829             else:\n-&gt; 4830                 new_axis = axis.drop(labels, errors=errors)\n   4831             indexer = axis.get_indexer(new_axis)\n   4832 \n\n/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py in drop(self, labels, errors)\n   7068         if mask.any():\n   7069             if errors != \"ignore\":\n-&gt; 7070                 raise KeyError(f\"{labels[mask].tolist()} not found in axis\")\n   7071             indexer = indexer[~mask]\n   7072         return self.delete(indexer)\n\nKeyError: \"['count'] not found in axis\"\n\n\n\n\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataframe (assuming it's already loaded as 'df_bikes').\n# df_bikes = pd.read_csv('your_data_source.csv')\n\n# Create the target variable by combining 'casual' and 'registered'\ndf_bikes['total_count'] = df_bikes['casual'] + df_bikes['registered']\n\n# Define features and target\nX = df_bikes[['temp_c', 'feels_like_c', 'hum', 'windspeed', 'weathersit', 'season', 'holiday', 'workingday']]\ny = df_bikes['total_count']\n\n# One-hot encode categorical features\nX = pd.get_dummies(X, columns=['weathersit', 'season', 'holiday', 'workingday'])\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the model\nmodel = keras.Sequential([\n    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(1)  # Output layer for regression\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mse', metrics=['mae'])\n\n# Train the model\nmodel.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n\n# Evaluate the model\nloss, mae = model.evaluate(X_test, y_test)\nprint(f\"Mean Absolute Error: {mae}\")\n\n# Make predictions\npredictions = model.predict(X_test)\n\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\nEpoch 1/10\n2250/2250 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - loss: 113390.3125 - mae: 243.1920 - val_loss: 85956.3984 - val_mae: 215.8419\nEpoch 2/10\n2250/2250 ━━━━━━━━━━━━━━━━━━━━ 12s 3ms/step - loss: 85492.3828 - mae: 217.6653 - val_loss: 81624.4141 - val_mae: 208.2635\nEpoch 3/10\n2250/2250 ━━━━━━━━━━━━━━━━━━━━ 6s 3ms/step - loss: 82931.4609 - mae: 212.6183 - val_loss: 79374.5156 - val_mae: 203.1673\nEpoch 4/10\n2250/2250 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - loss: 80729.6719 - mae: 209.0209 - val_loss: 78288.8438 - val_mae: 200.8338\nEpoch 5/10\n2250/2250 ━━━━━━━━━━━━━━━━━━━━ 7s 3ms/step - loss: 79184.2734 - mae: 206.5372 - val_loss: 78544.4688 - val_mae: 200.3522\nEpoch 6/10\n2250/2250 ━━━━━━━━━━━━━━━━━━━━ 6s 3ms/step - loss: 78554.8672 - mae: 205.1937 - val_loss: 78627.8906 - val_mae: 210.7880\nEpoch 7/10\n2250/2250 ━━━━━━━━━━━━━━━━━━━━ 7s 3ms/step - loss: 78600.2188 - mae: 205.5919 - val_loss: 77284.9609 - val_mae: 204.3620\nEpoch 8/10\n2250/2250 ━━━━━━━━━━━━━━━━━━━━ 9s 3ms/step - loss: 78882.2812 - mae: 206.0225 - val_loss: 77832.2656 - val_mae: 199.8365\nEpoch 9/10\n2250/2250 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - loss: 78725.6406 - mae: 205.3156 - val_loss: 78041.6094 - val_mae: 197.7063\nEpoch 10/10\n2250/2250 ━━━━━━━━━━━━━━━━━━━━ 11s 3ms/step - loss: 77927.5078 - mae: 203.9755 - val_loss: 76924.0156 - val_mae: 202.6045\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 76629.5703 - mae: 202.7970\nMean Absolute Error: 203.04978942871094\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Assuming you have trained your model and it's named 'model'\n# and your features are in a DataFrame called 'X'\n\n# Get feature importances using permutation importance\nfrom sklearn.inspection import permutation_importance\nresults = permutation_importance(model, X_test, y_test, scoring='neg_mean_squared_error')\n\n# Sort feature importances in descending order\nimportance = results.importances_mean\nsorted_idx = importance.argsort()[::-1] # Reverse the order to get descending sort\n\n# Create horizontal bar plot\nfig, ax = plt.subplots(figsize=(10, 6)) # Adjust figsize as needed\nax.barh(X.columns[sorted_idx], importance[sorted_idx])\nax.set_xlabel(\"Permutation Importance\")\nax.set_title(\"Feature Importance\")\nplt.show()\n\n703/703 ━━━━━━━━━━━━━━━━━━━━ 2s 3ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 2s 3ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n703/703 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "story_telling.html",
    "href": "story_telling.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Story Telling"
    ]
  },
  {
    "objectID": "story_telling.html#title-2-header",
    "href": "story_telling.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Story Telling"
    ]
  },
  {
    "objectID": "notebooks/module03_housing_grading_mini.html",
    "href": "notebooks/module03_housing_grading_mini.html",
    "title": "Alex Tovar - Data Science Portfolio",
    "section": "",
    "text": "Prep work:\n\nDownload team csv predictions file\nRename files to team8-module3-predictions.csv where team8 is the name of your team\nMake sure file is one column and remove any extra columns\nMake sure the heading is set to “price” (without quotes)\nUpload csv predictions to session storage area.\nClick the folder icon, then click the upload icon (paper with an upward arrow)\nRun the notebook (Runtime -&gt; Run all)\n\n\n# MODULE 03 - HOUSING HOLDOUT GRADING\n\nfrom pathlib import Path\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import root_mean_squared_error, mean_squared_error, r2_score, mean_absolute_error, median_absolute_error\n\n# READ IN THE CSV FILES\nteam_dir = Path(\"./\")\nteams = team_dir.glob(\"*-predictions.csv\")\nteam_list = []\nfor team in teams:\n  # print(latent_file)\n  team_list.append((str(team).split(\"-\",1)[0],team))\n\n# print(team_list)\n\n\n# READ IN THE HOLDOUT ANSWERS\ntargets_file = \"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/housing_holdout_test_mini_answers.csv\"\ntargets = pd.read_csv(targets_file)\n# targets\n\n\n# ARE THE STUDENT DATASETS THE CORRECT LENGTH\nstudent_datasets = {}\nfor (group, file) in team_list:\n  ds = pd.read_csv(file)\n\n  if ds.shape != targets.shape:\n    print(f\"Error group {group} ds had {ds.shape} rows and columns instead of the expected {targets.shape}. It will be excluded.\")\n  else:\n    student_datasets[group] = ds\n    print(f\"Group {group} added successfully\")\n\nGroup team8 added successfully\n\n\n\n# student_datasets\n\n\nresults_dict = {}\n\n\nfor group, student_ds in student_datasets.items():\n  student_dict = {}\n  # print(group,cm)\n  student_ds.columns=['price']\n  mse = root_mean_squared_error(targets, student_ds)\n  # print(\"{} - RMSE: {}\".format(group, mse))\n\n  student_dict[\"RMSE\"] = root_mean_squared_error(targets, student_ds)\n  student_dict[\"Mean Abs Error\"] = mean_absolute_error(targets, student_ds)\n  student_dict[\"Median Abs Error\"] = median_absolute_error(targets, student_ds)\n  student_dict[\"R2\"] = r2_score(targets, student_ds)\n\n  combined = pd.concat([targets, student_ds], axis=1)\n  combined.columns = [\"target\", \"pred\"]\n  combined[\"absdiff\"] = (combined[\"target\"] - combined[\"pred\"]).abs()\n  combined[\"absdiff_pct\"] = combined[\"absdiff\"] / combined[\"target\"]\n\n  shower = pd.DataFrame(student_ds, columns = ['price'])\n  shower.columns = ['predictions']\n  testfinal = pd.concat([shower,targets['price']],axis=1)\n  testfinal['difference'] = testfinal['price']-testfinal['predictions']\n  testfinal['percent_difference'] = abs(testfinal['difference']/testfinal['price'])\n  testfinal['percent_bucket'] = [ \"above 20%\" if i &gt;= 0.2 else \"below 20%\" for i in testfinal.percent_difference ]\n\n  # testfinal\n\n  student_dict['dataset'] = testfinal\n  percents = [5, 10, 20]\n  for percent in percents:\n    student_dict[f\"Percent of houses within {percent} percent\"] = len(combined[combined[\"absdiff_pct\"] &lt;= (percent /100)]) / len(combined) * 100\n\n  results_dict[group] = student_dict\n\n\n# df['predictions']\n\n\n# results_dict\n\n\nresults_df = pd.DataFrame(results_dict)\nresults_ds_trans = results_df.transpose()\nresults_ds_trans = results_ds_trans.drop(columns=[\"dataset\"])\n\nresults_ds_trans = results_ds_trans.round(2)\nresults_ds_trans = results_ds_trans.sort_values(by=\"R2\",ascending=False)\n\n\n# results_df\n\n\nsns.set(rc={'figure.figsize':(11.7,8.27)})\nfor team_results in results_dict.items():\n  testfinal = team_results[1]['dataset']\n  # print(team_results['dataset'])\n  # print(f\"R-Squared Value: {r2}\")\n  print(f\"-------------------------------- {team_results[0].upper()} RESULTS ---------------------------------\\n\")\n  print(f\" Within 5%: {team_results[1]['Percent of houses within 5 percent']}%\\n\",\n  f\"Within 10%: {team_results[1]['Percent of houses within 10 percent']}%\\n\",\n  f\"Within 20%: {team_results[1]['Percent of houses within 20 percent']}%\\n\",\n  f\"R^2: {team_results[1]['R2']}%\\n\",\n  f\"RMSE: {team_results[1]['RMSE']}\\n\",\n  f\"Mean Absolute Error: {team_results[1]['Mean Abs Error']}\\n\",\n  f\"Median Aboslute Error: {team_results[1]['Median Abs Error']}\")\n\n\n  color_dict = dict({'below 20%':'tab:blue',\n                    'above 20%': 'tab:orange'})\n  # print(testfinal['abspercentmiss'].describe(percentiles=[.1,.2,.3,.4,.5,.6,.7,.8,.9,.95]))\n  xlims=(0,4e6)\n  ylims=(0,4e6)\n  ax = sns.scatterplot(data=testfinal,x='price',y='predictions',hue=\"percent_bucket\",palette=color_dict)\n  # ax.set(xscale=\"log\", yscale=\"log\", xlim=xlims, ylim=ylims)\n  ax.plot(xlims,xlims, color='r')\n  # plt.legend(labels=['perfect',\"below 5\",'above 5','10-20%','above 20'])\n  plt.show()\n  print(f\"-\"*77)\n  print(\"\\n\"*3)\n\n-------------------------------- TEAM8 RESULTS ---------------------------------\n\n Within 5%: 30.864197530864196%\n Within 10%: 51.85185185185185%\n Within 20%: 82.71604938271605%\n R^2: 0.9069012577580781%\n RMSE: 87216.81001856703\n Mean Absolute Error: 57536.72839506173\n Median Aboslute Error: 38663.0\n\n\n\n\n\n\n\n\n\n-----------------------------------------------------------------------------\n\n\n\n\n\n\n\nresults_ds_trans = results_df.transpose()\n\nresults_ds_trans = results_ds_trans.drop(columns=[\"dataset\"])\nresults_ds_trans.to_csv(\"class_results.csv\")\nresults_ds_trans\n\n\n  \n    \n\n\n\n\n\n\nRMSE\nMean Abs Error\nMedian Abs Error\nR2\nPercent of houses within 5 percent\nPercent of houses within 10 percent\nPercent of houses within 20 percent\n\n\n\n\nteam8\n87216.810019\n57536.728395\n38663.0\n0.906901\n30.864198\n51.851852\n82.716049\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Full_Stack/project3.html",
    "href": "Full_Stack/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 3"
    ]
  },
  {
    "objectID": "Full_Stack/project1.html",
    "href": "Full_Stack/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 1"
    ]
  },
  {
    "objectID": "Full_Stack/project4.html",
    "href": "Full_Stack/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 4"
    ]
  },
  {
    "objectID": "competition.html",
    "href": "competition.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Competition"
    ]
  },
  {
    "objectID": "competition.html#title-2-header",
    "href": "competition.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Competition"
    ]
  },
  {
    "objectID": "Cleansing_Exploration/project5.html",
    "href": "Cleansing_Exploration/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project2.html",
    "href": "Cleansing_Exploration/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Projects/project3.html",
    "href": "Cleansing_Projects/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 3"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project1.html",
    "href": "Cleansing_Projects/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 1"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project4.html",
    "href": "Cleansing_Projects/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 4"
    ]
  },
  {
    "objectID": "Competition/project3.html",
    "href": "Competition/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 3"
    ]
  },
  {
    "objectID": "Competition/project1.html",
    "href": "Competition/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 1"
    ]
  },
  {
    "objectID": "Competition/project4.html",
    "href": "Competition/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 4"
    ]
  },
  {
    "objectID": "Story_Telling/project5.html",
    "href": "Story_Telling/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 5"
    ]
  },
  {
    "objectID": "Story_Telling/project2.html",
    "href": "Story_Telling/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 2"
    ]
  },
  {
    "objectID": "full_stack.html",
    "href": "full_stack.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Full Stack"
    ]
  },
  {
    "objectID": "full_stack.html#title-2-header",
    "href": "full_stack.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Full Stack"
    ]
  },
  {
    "objectID": "exploration.html",
    "href": "exploration.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "exploration.html#title-2-header",
    "href": "exploration.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "IceChallenge.html",
    "href": "IceChallenge.html",
    "title": "Alex Tovar - Data Science Portfolio",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# 1. Load and preprocess data\ntraining_data = pd.read_csv('/content/energy-training.csv')\nX = training_data.drop('Appliances', axis=1)\ny = training_data['Appliances']\n\n# 1a. Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 1b. Scale features using StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# 2. Build and train the model\nmodel = keras.Sequential([\n    layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # Increased neurons\n    layers.Dense(64, activation='relu'),  # Increased neurons\n    layers.Dense(1)\n])\nmodel.compile(optimizer='adam', loss='mse', metrics=['mae'])\n\n# Utilize more computer resources (if available)\n# Use a GPU if available\n# with tf.device('/GPU:0'):  # Replace '0' with the appropriate GPU index if you have multiple GPUs\nmodel.fit(X_train, y_train, epochs=200, batch_size=64, validation_split=0.2)  # Increased epochs and batch size\n\n# 3. Evaluate performance on the test set\ny_pred = model.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Mean Squared Error (MSE): {mse}\")\nprint(f\"Mean Absolute Error (MAE): {mae}\")\nprint(f\"R-squared (R2): {r2}\")\n\n# 4. Load and preprocess holdout data (for prediction)\nholdout_data = pd.read_csv('/content/energy-holdout.csv')\nX_holdout = scaler.transform(holdout_data)\n\n# 5. Predict 'Appliances' for the holdout set\nholdout_predictions = model.predict(X_holdout)\n\n# 6. Add predictions to the holdout DataFrame, preserving order\nholdout_data['Appliances'] = holdout_predictions\n\n# 7. Save predictions to CSV\nholdout_data[['Appliances']].to_csv('holdout_predictions.csv', index=False)  # Save only 'Appliances' column\n\nprint(\"Predictions saved to holdout_predictions.csv\")\n\nEpoch 1/200\n\n\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n139/139 ━━━━━━━━━━━━━━━━━━━━ 8s 20ms/step - loss: 16919.4844 - mae: 84.1111 - val_loss: 11292.1514 - val_mae: 58.0253\nEpoch 2/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - loss: 10157.6133 - mae: 57.3960 - val_loss: 10449.8486 - val_mae: 55.1382\nEpoch 3/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - loss: 9558.1035 - mae: 54.5558 - val_loss: 10125.6094 - val_mae: 55.6274\nEpoch 4/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 9263.1748 - mae: 54.2836 - val_loss: 9902.0146 - val_mae: 53.2136\nEpoch 5/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 9288.0244 - mae: 53.7892 - val_loss: 9828.7100 - val_mae: 56.4782\nEpoch 6/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 8685.1797 - mae: 52.1939 - val_loss: 9682.0625 - val_mae: 55.4178\nEpoch 7/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 8054.5220 - mae: 50.0185 - val_loss: 9569.9941 - val_mae: 54.7225\nEpoch 8/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 8294.4219 - mae: 51.0906 - val_loss: 9508.5293 - val_mae: 56.3457\nEpoch 9/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 8313.5615 - mae: 51.5350 - val_loss: 9445.8291 - val_mae: 54.1347\nEpoch 10/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - loss: 8298.2803 - mae: 50.3938 - val_loss: 9346.5371 - val_mae: 52.7302\nEpoch 11/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 8333.1729 - mae: 50.6396 - val_loss: 9289.0459 - val_mae: 52.1619\nEpoch 12/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 8510.2676 - mae: 50.9303 - val_loss: 9281.7676 - val_mae: 54.5798\nEpoch 13/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 8385.4912 - mae: 51.5420 - val_loss: 9251.7959 - val_mae: 53.3166\nEpoch 14/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 8007.3652 - mae: 50.1129 - val_loss: 9204.0254 - val_mae: 50.7214\nEpoch 15/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 7989.9419 - mae: 48.6868 - val_loss: 9198.6992 - val_mae: 53.8437\nEpoch 16/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 8152.6270 - mae: 49.9745 - val_loss: 9144.7910 - val_mae: 50.5694\nEpoch 17/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 8271.0117 - mae: 49.8804 - val_loss: 9151.8057 - val_mae: 51.7687\nEpoch 18/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 8424.2461 - mae: 50.4785 - val_loss: 9101.1768 - val_mae: 52.6139\nEpoch 19/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 8162.9043 - mae: 50.4896 - val_loss: 9042.3213 - val_mae: 51.8121\nEpoch 20/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 7993.4126 - mae: 49.9721 - val_loss: 9007.7949 - val_mae: 52.2219\nEpoch 21/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 8226.2520 - mae: 50.2551 - val_loss: 8979.8750 - val_mae: 49.8709\nEpoch 22/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 7058.6836 - mae: 46.9696 - val_loss: 8996.8379 - val_mae: 51.8872\nEpoch 23/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 8162.1592 - mae: 50.4066 - val_loss: 8928.0010 - val_mae: 51.5303\nEpoch 24/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 8056.4277 - mae: 49.0593 - val_loss: 8991.5205 - val_mae: 52.1557\nEpoch 25/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 7774.6602 - mae: 48.7094 - val_loss: 8970.4180 - val_mae: 52.0192\nEpoch 26/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 8078.0859 - mae: 49.9559 - val_loss: 8839.9150 - val_mae: 49.6531\nEpoch 27/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 7855.9092 - mae: 47.7237 - val_loss: 8787.2783 - val_mae: 51.7349\nEpoch 28/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 7921.2534 - mae: 49.1936 - val_loss: 8755.8467 - val_mae: 51.7181\nEpoch 29/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 7842.0044 - mae: 48.8523 - val_loss: 8713.1045 - val_mae: 51.2081\nEpoch 30/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - loss: 7300.4146 - mae: 46.7909 - val_loss: 8692.4736 - val_mae: 50.2401\nEpoch 31/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - loss: 7521.4492 - mae: 48.1866 - val_loss: 8629.2783 - val_mae: 50.4965\nEpoch 32/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 7275.3721 - mae: 46.7603 - val_loss: 8632.1084 - val_mae: 48.3840\nEpoch 33/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 7372.0498 - mae: 47.8663 - val_loss: 8605.4199 - val_mae: 51.4824\nEpoch 34/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 7287.8428 - mae: 47.3101 - val_loss: 8681.7881 - val_mae: 53.0578\nEpoch 35/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 7286.5576 - mae: 47.6684 - val_loss: 8586.6875 - val_mae: 52.0667\nEpoch 36/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 7057.9570 - mae: 47.0983 - val_loss: 8539.7354 - val_mae: 51.3108\nEpoch 37/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 7625.8184 - mae: 47.8845 - val_loss: 8513.4092 - val_mae: 50.3586\nEpoch 38/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 7026.9961 - mae: 46.0065 - val_loss: 8529.5742 - val_mae: 51.6741\nEpoch 39/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 7466.8750 - mae: 47.7957 - val_loss: 8448.6230 - val_mae: 48.4213\nEpoch 40/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 7374.9492 - mae: 46.6653 - val_loss: 8516.5010 - val_mae: 51.5877\nEpoch 41/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 6982.2778 - mae: 45.9360 - val_loss: 8450.5977 - val_mae: 51.0922\nEpoch 42/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 7075.5288 - mae: 46.8926 - val_loss: 8417.7793 - val_mae: 49.3608\nEpoch 43/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 7215.4512 - mae: 46.7294 - val_loss: 8486.5723 - val_mae: 49.1804\nEpoch 44/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 6920.4863 - mae: 45.7040 - val_loss: 8374.7412 - val_mae: 50.1712\nEpoch 45/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 6632.9775 - mae: 45.5644 - val_loss: 8532.3633 - val_mae: 52.2835\nEpoch 46/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 6682.3848 - mae: 46.3150 - val_loss: 8408.8750 - val_mae: 50.0284\nEpoch 47/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 7102.9512 - mae: 46.0667 - val_loss: 8426.0723 - val_mae: 50.7263\nEpoch 48/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 6450.7573 - mae: 44.7674 - val_loss: 8326.8604 - val_mae: 49.3058\nEpoch 49/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 6918.5332 - mae: 46.1050 - val_loss: 8282.3164 - val_mae: 49.8209\nEpoch 50/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 6900.2759 - mae: 46.5505 - val_loss: 8342.0059 - val_mae: 49.4670\nEpoch 51/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 6712.5649 - mae: 45.2309 - val_loss: 8342.0176 - val_mae: 51.2399\nEpoch 52/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - loss: 6778.9575 - mae: 45.6440 - val_loss: 8251.9346 - val_mae: 49.0944\nEpoch 53/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - loss: 6757.7305 - mae: 45.0385 - val_loss: 8306.4521 - val_mae: 50.3431\nEpoch 54/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 6502.5093 - mae: 45.0637 - val_loss: 8286.8203 - val_mae: 49.6268\nEpoch 55/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 6389.4292 - mae: 43.8687 - val_loss: 8269.8799 - val_mae: 49.2479\nEpoch 56/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 7147.8672 - mae: 46.5142 - val_loss: 8246.8350 - val_mae: 50.6866\nEpoch 57/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 6527.9707 - mae: 45.3003 - val_loss: 8260.6719 - val_mae: 50.7183\nEpoch 58/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 6270.4233 - mae: 44.3391 - val_loss: 8180.0781 - val_mae: 49.8501\nEpoch 59/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 6582.6440 - mae: 45.1952 - val_loss: 8284.8945 - val_mae: 52.2255\nEpoch 60/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 6531.3184 - mae: 45.5129 - val_loss: 8236.0234 - val_mae: 50.5258\nEpoch 61/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 6138.4028 - mae: 43.8709 - val_loss: 8145.9668 - val_mae: 49.9585\nEpoch 62/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 6419.1338 - mae: 44.8574 - val_loss: 8145.1035 - val_mae: 49.3162\nEpoch 63/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 6747.7275 - mae: 45.3120 - val_loss: 8195.5527 - val_mae: 49.3024\nEpoch 64/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 6455.7515 - mae: 45.0634 - val_loss: 8141.4229 - val_mae: 48.8221\nEpoch 65/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 6816.4663 - mae: 45.7517 - val_loss: 8155.7124 - val_mae: 50.0629\nEpoch 66/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 6734.5596 - mae: 46.0778 - val_loss: 8287.2119 - val_mae: 51.6077\nEpoch 67/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 6598.1587 - mae: 44.5952 - val_loss: 8262.2803 - val_mae: 50.2750\nEpoch 68/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 6840.3843 - mae: 46.2842 - val_loss: 8188.1387 - val_mae: 48.5737\nEpoch 69/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 6610.2461 - mae: 45.5819 - val_loss: 8137.2725 - val_mae: 49.9233\nEpoch 70/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 6104.7119 - mae: 44.0720 - val_loss: 8110.0996 - val_mae: 48.4684\nEpoch 71/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 6353.9761 - mae: 44.5689 - val_loss: 8116.9810 - val_mae: 48.2437\nEpoch 72/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 6277.6162 - mae: 44.4457 - val_loss: 8076.5864 - val_mae: 48.5406\nEpoch 73/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - loss: 5756.1743 - mae: 42.3799 - val_loss: 8389.6875 - val_mae: 53.1435\nEpoch 74/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - loss: 6548.3174 - mae: 45.5989 - val_loss: 8057.4111 - val_mae: 47.5777\nEpoch 75/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 6171.2383 - mae: 43.6352 - val_loss: 8083.6860 - val_mae: 49.5166\nEpoch 76/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5855.1763 - mae: 42.9635 - val_loss: 8001.3892 - val_mae: 47.9447\nEpoch 77/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 6449.9946 - mae: 45.0596 - val_loss: 8158.1338 - val_mae: 47.4062\nEpoch 78/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 6363.4468 - mae: 44.1882 - val_loss: 8079.0942 - val_mae: 48.6686\nEpoch 79/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 6126.6313 - mae: 43.3937 - val_loss: 8263.5059 - val_mae: 54.3357\nEpoch 80/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5912.4717 - mae: 43.4406 - val_loss: 8153.1860 - val_mae: 49.4222\nEpoch 81/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 6437.6714 - mae: 44.7094 - val_loss: 8075.2017 - val_mae: 47.6817\nEpoch 82/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 5809.5371 - mae: 43.1121 - val_loss: 8243.4590 - val_mae: 53.1069\nEpoch 83/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 6068.6904 - mae: 44.1453 - val_loss: 8180.1099 - val_mae: 50.2271\nEpoch 84/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 5977.8193 - mae: 43.4651 - val_loss: 8226.1484 - val_mae: 51.8220\nEpoch 85/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 5598.9600 - mae: 42.3419 - val_loss: 8141.6177 - val_mae: 50.0789\nEpoch 86/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 5581.4648 - mae: 42.5872 - val_loss: 8095.1494 - val_mae: 47.8000\nEpoch 87/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 5759.6157 - mae: 42.6959 - val_loss: 8108.7007 - val_mae: 49.6191\nEpoch 88/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 5699.8154 - mae: 43.2459 - val_loss: 8027.7788 - val_mae: 49.4456\nEpoch 89/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5815.9512 - mae: 42.7044 - val_loss: 8005.4893 - val_mae: 50.1206\nEpoch 90/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 5982.1875 - mae: 43.6572 - val_loss: 8121.2349 - val_mae: 49.2487\nEpoch 91/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 6151.3779 - mae: 44.0424 - val_loss: 8144.0083 - val_mae: 53.9490\nEpoch 92/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5849.7808 - mae: 43.6050 - val_loss: 8146.9980 - val_mae: 53.7287\nEpoch 93/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - loss: 6047.8140 - mae: 44.6060 - val_loss: 7978.3706 - val_mae: 48.6240\nEpoch 94/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - loss: 5893.5635 - mae: 42.4641 - val_loss: 8008.1348 - val_mae: 49.6838\nEpoch 95/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - loss: 5774.2773 - mae: 42.9954 - val_loss: 7975.1323 - val_mae: 48.7938\nEpoch 96/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 6071.5591 - mae: 44.1517 - val_loss: 7951.0137 - val_mae: 49.3879\nEpoch 97/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5925.3774 - mae: 43.6231 - val_loss: 8042.6860 - val_mae: 49.0461\nEpoch 98/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5292.1353 - mae: 41.1153 - val_loss: 8152.6646 - val_mae: 52.9900\nEpoch 99/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 5717.6128 - mae: 43.4870 - val_loss: 7960.2402 - val_mae: 48.8650\nEpoch 100/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5386.2852 - mae: 41.3665 - val_loss: 8018.5010 - val_mae: 49.9066\nEpoch 101/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5890.9971 - mae: 43.1670 - val_loss: 7927.0747 - val_mae: 49.8785\nEpoch 102/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5738.5254 - mae: 42.4332 - val_loss: 7991.0952 - val_mae: 48.5538\nEpoch 103/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 5771.3667 - mae: 42.8712 - val_loss: 7907.0835 - val_mae: 49.0924\nEpoch 104/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5424.0059 - mae: 41.5374 - val_loss: 8154.9092 - val_mae: 51.1137\nEpoch 105/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 5719.9150 - mae: 43.7480 - val_loss: 8021.4497 - val_mae: 50.1824\nEpoch 106/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5469.0078 - mae: 42.4867 - val_loss: 7931.5942 - val_mae: 48.9583\nEpoch 107/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5363.0205 - mae: 41.6529 - val_loss: 8047.8291 - val_mae: 50.2324\nEpoch 108/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5662.9229 - mae: 42.7208 - val_loss: 8030.6934 - val_mae: 51.6784\nEpoch 109/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5453.8589 - mae: 41.9325 - val_loss: 8097.0781 - val_mae: 50.4769\nEpoch 110/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 5414.8042 - mae: 42.2434 - val_loss: 8104.5674 - val_mae: 52.0499\nEpoch 111/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5280.7427 - mae: 41.7166 - val_loss: 7997.0352 - val_mae: 50.6827\nEpoch 112/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5697.6416 - mae: 43.0679 - val_loss: 8087.6250 - val_mae: 49.9017\nEpoch 113/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5876.1025 - mae: 43.3276 - val_loss: 7935.4712 - val_mae: 50.3441\nEpoch 114/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5865.1436 - mae: 42.9454 - val_loss: 8054.9639 - val_mae: 48.7424\nEpoch 115/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5402.7178 - mae: 41.3467 - val_loss: 7902.9614 - val_mae: 48.3611\nEpoch 116/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 5332.6943 - mae: 41.4098 - val_loss: 8054.5728 - val_mae: 49.1914\nEpoch 117/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5476.5527 - mae: 41.8528 - val_loss: 7984.1904 - val_mae: 51.0058\nEpoch 118/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 5749.7363 - mae: 42.9595 - val_loss: 7983.3428 - val_mae: 48.4737\nEpoch 119/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 5459.0112 - mae: 41.4611 - val_loss: 7855.5513 - val_mae: 48.3523\nEpoch 120/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 5242.3579 - mae: 41.7851 - val_loss: 7932.0181 - val_mae: 50.1608\nEpoch 121/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 5267.0415 - mae: 41.2207 - val_loss: 7975.3257 - val_mae: 50.7399\nEpoch 122/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5569.3584 - mae: 42.0274 - val_loss: 7945.7129 - val_mae: 49.5791\nEpoch 123/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 5575.2153 - mae: 42.3081 - val_loss: 7862.7275 - val_mae: 49.2778\nEpoch 124/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 5045.1069 - mae: 40.4137 - val_loss: 7946.4570 - val_mae: 50.1174\nEpoch 125/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4984.1943 - mae: 40.5995 - val_loss: 7929.8066 - val_mae: 50.7216\nEpoch 126/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 5521.5103 - mae: 42.8719 - val_loss: 7905.4946 - val_mae: 49.4972\nEpoch 127/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 5105.8550 - mae: 41.2967 - val_loss: 7910.4028 - val_mae: 49.6587\nEpoch 128/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4798.8188 - mae: 39.6810 - val_loss: 7858.7471 - val_mae: 49.4054\nEpoch 129/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 5315.5986 - mae: 41.3057 - val_loss: 7983.0718 - val_mae: 50.2678\nEpoch 130/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5493.1768 - mae: 42.8445 - val_loss: 7934.6943 - val_mae: 51.5800\nEpoch 131/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4789.5962 - mae: 39.4207 - val_loss: 7864.2778 - val_mae: 50.3411\nEpoch 132/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4720.7856 - mae: 39.8893 - val_loss: 8151.4883 - val_mae: 52.3285\nEpoch 133/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - loss: 5347.3208 - mae: 42.3164 - val_loss: 8008.8311 - val_mae: 51.5311\nEpoch 134/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - loss: 5481.1792 - mae: 42.4164 - val_loss: 7857.1943 - val_mae: 49.6211\nEpoch 135/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - loss: 5705.3560 - mae: 43.1522 - val_loss: 7854.5825 - val_mae: 49.0653\nEpoch 136/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - loss: 5427.1553 - mae: 41.9625 - val_loss: 7965.5874 - val_mae: 50.0863\nEpoch 137/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 5226.2261 - mae: 41.3941 - val_loss: 7826.3687 - val_mae: 48.5786\nEpoch 138/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 5139.3188 - mae: 41.4281 - val_loss: 7890.4731 - val_mae: 49.8334\nEpoch 139/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 5052.2192 - mae: 40.6321 - val_loss: 7785.1870 - val_mae: 49.9419\nEpoch 140/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4893.3320 - mae: 40.1754 - val_loss: 7759.4072 - val_mae: 48.1112\nEpoch 141/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 5071.3110 - mae: 40.8855 - val_loss: 7961.3706 - val_mae: 50.4298\nEpoch 142/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 5483.4683 - mae: 42.5893 - val_loss: 7933.2500 - val_mae: 49.0426\nEpoch 143/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4951.2510 - mae: 40.6608 - val_loss: 7958.3911 - val_mae: 49.2787\nEpoch 144/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5362.4302 - mae: 42.1485 - val_loss: 7803.0200 - val_mae: 48.5522\nEpoch 145/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5007.1147 - mae: 40.6782 - val_loss: 7935.3579 - val_mae: 49.4630\nEpoch 146/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 4911.5361 - mae: 39.9068 - val_loss: 7886.4722 - val_mae: 49.1248\nEpoch 147/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4841.0117 - mae: 40.2432 - val_loss: 7845.9136 - val_mae: 48.8003\nEpoch 148/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4753.0454 - mae: 39.7188 - val_loss: 7835.7798 - val_mae: 50.4459\nEpoch 149/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5090.4707 - mae: 41.5976 - val_loss: 7988.5630 - val_mae: 50.6088\nEpoch 150/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 5020.4429 - mae: 40.6010 - val_loss: 7782.7808 - val_mae: 49.6457\nEpoch 151/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4892.8184 - mae: 40.2570 - val_loss: 7896.9795 - val_mae: 50.2041\nEpoch 152/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 4791.3335 - mae: 40.3859 - val_loss: 7759.8516 - val_mae: 49.0300\nEpoch 153/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 4920.2300 - mae: 40.9959 - val_loss: 7865.3887 - val_mae: 48.9904\nEpoch 154/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - loss: 5244.4722 - mae: 41.6608 - val_loss: 7764.2651 - val_mae: 49.9517\nEpoch 155/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4564.7197 - mae: 39.5746 - val_loss: 8032.5854 - val_mae: 52.0715\nEpoch 156/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - loss: 4953.6592 - mae: 40.9675 - val_loss: 8149.1348 - val_mae: 50.4579\nEpoch 157/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 5192.6572 - mae: 41.3784 - val_loss: 8134.2930 - val_mae: 53.6568\nEpoch 158/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4643.1987 - mae: 40.0253 - val_loss: 8245.2412 - val_mae: 53.8717\nEpoch 159/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4554.0645 - mae: 40.0233 - val_loss: 8007.7588 - val_mae: 50.5148\nEpoch 160/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4841.3418 - mae: 40.4654 - val_loss: 7856.2607 - val_mae: 50.6380\nEpoch 161/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 4898.4404 - mae: 41.1735 - val_loss: 7963.8496 - val_mae: 51.2158\nEpoch 162/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4814.2090 - mae: 40.6472 - val_loss: 7951.8760 - val_mae: 50.1792\nEpoch 163/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4728.6084 - mae: 40.1178 - val_loss: 7894.3472 - val_mae: 49.1435\nEpoch 164/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 4655.9155 - mae: 39.7979 - val_loss: 7897.4390 - val_mae: 49.8684\nEpoch 165/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 4479.6309 - mae: 39.8550 - val_loss: 8035.1367 - val_mae: 50.6336\nEpoch 166/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 4527.0933 - mae: 39.1583 - val_loss: 8143.2256 - val_mae: 53.2819\nEpoch 167/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 4946.5190 - mae: 40.8916 - val_loss: 7863.1133 - val_mae: 49.6618\nEpoch 168/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4559.6489 - mae: 39.3351 - val_loss: 7763.9404 - val_mae: 49.4567\nEpoch 169/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4472.4321 - mae: 39.3160 - val_loss: 8129.1045 - val_mae: 51.7523\nEpoch 170/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4754.3413 - mae: 40.5338 - val_loss: 7879.8955 - val_mae: 50.0638\nEpoch 171/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 4702.5874 - mae: 39.8963 - val_loss: 7807.5396 - val_mae: 50.1633\nEpoch 172/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4861.9106 - mae: 41.0930 - val_loss: 7813.4648 - val_mae: 49.2090\nEpoch 173/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 4438.4727 - mae: 39.1027 - val_loss: 8014.5825 - val_mae: 51.8586\nEpoch 174/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - loss: 4842.7808 - mae: 40.2788 - val_loss: 7886.8154 - val_mae: 50.7899\nEpoch 175/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - loss: 4547.6084 - mae: 39.7432 - val_loss: 8062.7983 - val_mae: 52.3661\nEpoch 176/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - loss: 4997.7642 - mae: 41.4260 - val_loss: 7846.3784 - val_mae: 50.5377\nEpoch 177/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4579.7256 - mae: 39.7806 - val_loss: 7800.3589 - val_mae: 49.8657\nEpoch 178/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4434.1138 - mae: 39.1035 - val_loss: 7867.9648 - val_mae: 50.8570\nEpoch 179/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4678.6045 - mae: 39.9512 - val_loss: 7803.1431 - val_mae: 49.1338\nEpoch 180/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 4856.2520 - mae: 40.3890 - val_loss: 7821.8130 - val_mae: 49.4777\nEpoch 181/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 4611.3550 - mae: 40.0992 - val_loss: 7910.6348 - val_mae: 50.7880\nEpoch 182/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 4732.2710 - mae: 39.9768 - val_loss: 7757.1743 - val_mae: 48.9449\nEpoch 183/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4515.2612 - mae: 39.6214 - val_loss: 7888.5566 - val_mae: 49.9183\nEpoch 184/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 4588.1733 - mae: 39.9021 - val_loss: 7737.2930 - val_mae: 49.5928\nEpoch 185/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4571.0830 - mae: 39.2246 - val_loss: 7865.3452 - val_mae: 51.3735\nEpoch 186/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 4361.1938 - mae: 39.3373 - val_loss: 7875.2637 - val_mae: 50.4986\nEpoch 187/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4562.4048 - mae: 39.3547 - val_loss: 7957.1089 - val_mae: 51.7982\nEpoch 188/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 4377.6855 - mae: 39.3381 - val_loss: 7946.9741 - val_mae: 50.9189\nEpoch 189/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 4433.2290 - mae: 39.3934 - val_loss: 7833.0703 - val_mae: 51.5004\nEpoch 190/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - loss: 4567.8232 - mae: 39.8874 - val_loss: 7977.7310 - val_mae: 52.3793\nEpoch 191/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4339.6382 - mae: 39.1236 - val_loss: 7942.0708 - val_mae: 51.3152\nEpoch 192/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4698.6934 - mae: 39.7167 - val_loss: 7887.2349 - val_mae: 52.4531\nEpoch 193/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 4415.8799 - mae: 38.8989 - val_loss: 7971.6680 - val_mae: 52.3898\nEpoch 194/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - loss: 4697.5317 - mae: 40.0345 - val_loss: 7954.6089 - val_mae: 50.5517\nEpoch 195/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - loss: 4244.6680 - mae: 38.6138 - val_loss: 7867.5449 - val_mae: 50.5358\nEpoch 196/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4265.2822 - mae: 38.3969 - val_loss: 7995.7690 - val_mae: 53.1810\nEpoch 197/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4723.9727 - mae: 40.7110 - val_loss: 7972.2363 - val_mae: 49.3264\nEpoch 198/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 4225.3638 - mae: 38.3328 - val_loss: 7731.9551 - val_mae: 49.6191\nEpoch 199/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4443.6851 - mae: 39.3060 - val_loss: 7847.8813 - val_mae: 49.9236\nEpoch 200/200\n139/139 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - loss: 4079.5369 - mae: 37.6485 - val_loss: 7746.6060 - val_mae: 49.6606\n87/87 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step\nMean Squared Error (MSE): 6041.79736328125\nMean Absolute Error (MAE): 45.84733200073242\nR-squared (R2): 0.3289639949798584\n183/183 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step\nPredictions saved to holdout_predictions.csv\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Machine_Learning/project5.html",
    "href": "Machine_Learning/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 5"
    ]
  },
  {
    "objectID": "Machine_Learning/project2.html",
    "href": "Machine_Learning/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 2"
    ]
  },
  {
    "objectID": "starter_signs_v2_student_using_advanced_technique_code_chunks.html",
    "href": "starter_signs_v2_student_using_advanced_technique_code_chunks.html",
    "title": "Alex Tovar - Data Science Portfolio",
    "section": "",
    "text": "# Note: After you run this cell, the training and test data will be available in\n# the file browser. (Click the folder icon on the left to view it)\n#\n# If you don't see the data after the cell completes, click the refresh button\n# in the file browser (folder icon with circular arrow)\n\n# First, let's download and unzip the data\n!echo \"Downloading files...\"\n!wget -q https://github.com/byui-cse/cse450-course/raw/master/data/roadsigns/training1.zip\n!wget -q https://github.com/byui-cse/cse450-course/raw/master/data/roadsigns/training2.zip\n!wget -q https://github.com/byui-cse/cse450-course/raw/master/data/roadsigns/holdout.zip\n!wget -q https://github.com/byui-cse/cse450-course/raw/master/data/roadsigns/mini_holdout.zip\n!wget -q https://github.com/byui-cse/cse450-course/raw/master/data/roadsigns/mini_holdout_answers.csv\n\n!echo \"Unzipping files...\"\n!unzip -q /content/training1.zip\n!unzip -q /content/training2.zip\n!unzip -q /content/holdout.zip\n!unzip -q /content/mini_holdout.zip\n\n# Combine the two traning directories\n!echo \"Merging training data...\"\n!mkdir /content/training\n!mv /content/training1/* /content/training\n!mv /content/training2/* /content/training\n\n# Cleanup\n!echo \"Cleaning up...\"\n!rmdir /content/training1\n!rmdir /content/training2\n!rm training1.zip\n!rm training2.zip\n!rm holdout.zip\n!rm mini_holdout.zip\n\n!echo \"Data ready.\"\n\nDownloading files...\nUnzipping files...\nMerging training data...\nCleaning up...\nData ready.\n\n\n\n# Import libraries\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# We're using keras' ImageDataGenerator class to load our image data.\n# See (https://keras.io/api/preprocessing/image/#imagedatagenerator-class) for details\n#\n# A couple of things to note:\n# 1. We're specifying a number for the seed, so we'll always get the same shuffle and split of our images.\n# 2. Class names are inferred automatically from the image subdirectory names.\n# 3. We're splitting the training data into 80% training, 20% validation.\n\n\ntraining_dir = '/content/training/'\nimage_size = (100, 100)\n\n# Split up the training data images into training and validations sets\n# We'll use and ImageDataGenerator to do the splits\n# ImageDataGenerator can also be used to do preprocessing and agumentation on the files as can be seen with rescale\n\ntrain_datagen = ImageDataGenerator(\n        rescale=1./255,\n        validation_split=.2\n        )\nvalidation_datagen = ImageDataGenerator(\n        rescale=1./255,\n        validation_split=.2\n        )\n\ntrain_generator = train_datagen.flow_from_directory(\n        training_dir,\n        target_size = image_size,\n        subset=\"training\",\n        batch_size=32,\n        class_mode='sparse',\n        seed=42,shuffle=True)\nvalidation_generator = validation_datagen.flow_from_directory(\n        training_dir,\n        target_size=image_size,\n        batch_size=32,\n        class_mode='sparse',\n        subset=\"validation\",\n        seed=42)\n\n\nFound 31368 images belonging to 43 classes.\nFound 7841 images belonging to 43 classes.\n\n\n\n#these might come in handy\ntarget_names = ['Speed_20', 'Speed_30', 'Speed_50', 'Speed_60', 'Speed_70',\n               'Speed_80','Speed_Limit_Ends', 'Speed_100', 'Speed_120', 'Overtaking_Prohibited',\n               'Overtakeing_Prohibited_Trucks', 'Crossroad_Ahead', 'Priority_Road_Ahead', 'Yield', 'STOP',\n               'Entry_Forbidden', 'Trucks_Forbidden', 'No_Entry(one-way traffic)', 'Cars_Prohibited(!)', 'Left_Curve_Ahead',\n               'Right_Curve_Ahead', 'Bends_Left_Then_Right', 'Poor_Surface_Ahead', 'Slippery_Surface_Ahead', 'Road_Narrows_On_Right',\n               'Roadwork_Ahead', 'Traffic_Light_Ahead', 'Warning_Pedestrians', 'Warning_Children', 'Warning_Bikes',\n               'Uncontrolled_Crossroad', 'Deer_Crossing', 'End_Previous_Limitation', 'Turning_Right_Compulsory', 'Turning_Left_Compulsory',\n               'Ahead_Only', 'Straight_Or_Right_Mandatory', 'Straight_Or_Left_Mandatory', 'Passing_Right_Compulsory', 'Passing_Left_Compulsory',\n               'Roundabout', 'End_Overtaking_Prohibition', 'End_Overtaking_Prohibition_Trucks']\n\n\n# View 9 images and their class labels\nplt.figure(figsize=(10, 10))\nimages, labels = next(train_generator)  # Assuming train_generator is a generator\nbatch_size = images.shape[0]\n\nfor i in range(min(9, batch_size)):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow((images[i] * 255).astype(\"uint8\"))\n    plt.title(int(labels[i]))\n    plt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Enhanced data augmentation for traffic sign images\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=10,        # Slight rotations (traffic signs should still be upright)\n    width_shift_range=0.1,    # Small horizontal shifts\n    height_shift_range=0.1,   # Small vertical shifts\n    zoom_range=0.1,           # Slight zooming in/out\n    brightness_range=(0.8, 1.2),  # Brightness variations\n    shear_range=5,            # Slight shearing\n    fill_mode='nearest',\n    validation_split=0.2\n)\n\n\n# We still need basic rescaling for validation data, but no augmentation\nvalidation_datagen = ImageDataGenerator(\n    rescale=1./255,\n    validation_split=0.2\n)\n\n\n# Update the generators with the new augmentation settings\ntrain_generator = train_datagen.flow_from_directory(\n    training_dir,\n    target_size=image_size,\n    batch_size=32,\n    class_mode='sparse',\n    subset=\"training\",\n    seed=42\n)\n\nvalidation_generator = validation_datagen.flow_from_directory(\n    training_dir,\n    target_size=image_size,\n    batch_size=32,\n    class_mode='sparse',\n    subset=\"validation\",\n    seed=42\n)\n\nFound 31368 images belonging to 43 classes.\nFound 7841 images belonging to 43 classes.\n\n\n\n# Visualize some augmented images to verify the augmentation\nplt.figure(figsize=(12, 8))\nfor i in range(9):\n    augmented_images, labels = next(train_generator)\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(augmented_images[0])\n    plt.title(f\"Class: {target_names[int(labels[0])]}\")\n    plt.axis(\"off\")\nplt.tight_layout()\nplt.suptitle(\"Examples of Augmented Training Images\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# More libraries to further improve the model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n\n\n# Define a more advanced CNN architecture\nimproved_model = Sequential([\n    # First convolutional block\n    Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(100, 100, 3)),\n    BatchNormalization(),\n    Conv2D(32, (3, 3), padding='same', activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D(pool_size=(2, 2)),\n    Dropout(0.2),\n\n    # Second convolutional block\n    Conv2D(64, (3, 3), padding='same', activation='relu'),\n    BatchNormalization(),\n    Conv2D(64, (3, 3), padding='same', activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D(pool_size=(2, 2)),\n    Dropout(0.3),\n\n    # Third convolutional block\n    Conv2D(128, (3, 3), padding='same', activation='relu'),\n    BatchNormalization(),\n    Conv2D(128, (3, 3), padding='same', activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D(pool_size=(2, 2)),\n    Dropout(0.4),\n\n    # Flatten and dense layers\n    Flatten(),\n    Dense(512, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.5),\n    Dense(43, activation='softmax')  # 43 classes for German traffic signs\n])\n\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n\n# Compile the model with a lower learning rate\nimproved_model.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n\n# Print model summary\nimproved_model.summary()\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv2d (Conv2D)                      │ (None, 100, 100, 32)        │             896 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization                  │ (None, 100, 100, 32)        │             128 │\n│ (BatchNormalization)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_1 (Conv2D)                    │ (None, 100, 100, 32)        │           9,248 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_1                │ (None, 100, 100, 32)        │             128 │\n│ (BatchNormalization)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d (MaxPooling2D)         │ (None, 50, 50, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (Dropout)                    │ (None, 50, 50, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_2 (Conv2D)                    │ (None, 50, 50, 64)          │          18,496 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_2                │ (None, 50, 50, 64)          │             256 │\n│ (BatchNormalization)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_3 (Conv2D)                    │ (None, 50, 50, 64)          │          36,928 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_3                │ (None, 50, 50, 64)          │             256 │\n│ (BatchNormalization)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_1 (MaxPooling2D)       │ (None, 25, 25, 64)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (Dropout)                  │ (None, 25, 25, 64)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_4 (Conv2D)                    │ (None, 25, 25, 128)         │          73,856 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_4                │ (None, 25, 25, 128)         │             512 │\n│ (BatchNormalization)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_5 (Conv2D)                    │ (None, 25, 25, 128)         │         147,584 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_5                │ (None, 25, 25, 128)         │             512 │\n│ (BatchNormalization)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_2 (MaxPooling2D)       │ (None, 12, 12, 128)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_2 (Dropout)                  │ (None, 12, 12, 128)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten (Flatten)                    │ (None, 18432)               │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (Dense)                        │ (None, 512)                 │       9,437,696 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_6                │ (None, 512)                 │           2,048 │\n│ (BatchNormalization)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (Dropout)                  │ (None, 512)                 │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (Dense)                      │ (None, 43)                  │          22,059 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 9,750,603 (37.20 MB)\n\n\n\n Trainable params: 9,748,683 (37.19 MB)\n\n\n\n Non-trainable params: 1,920 (7.50 KB)\n\n\n\n\n# Define callbacks to improve training\ncallbacks = [\n    # Reduce learning rate when validation accuracy plateaus\n    ReduceLROnPlateau(\n        monitor='val_accuracy',\n        factor=0.5,\n        patience=3,\n        min_lr=1e-6,\n        verbose=1\n    ),\n\n    # Early stopping to prevent overfitting\n    EarlyStopping(\n        monitor='val_accuracy',\n        patience=10,\n        restore_best_weights=True,\n        verbose=1\n    ),\n\n    # Save the best model\n    ModelCheckpoint(\n        'best_traffic_sign_model.h5',\n        monitor='val_accuracy',\n        save_best_only=True,\n        verbose=1\n    )\n]\n\n\n# Train the model with callbacks\nhistory = improved_model.fit(\n    train_generator,\n    validation_data=validation_generator,\n    epochs=50,  # We'll use early stopping to determine actual epochs\n    callbacks=callbacks\n)\n\n/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n\n\nEpoch 1/50\n981/981 ━━━━━━━━━━━━━━━━━━━━ 0s 128ms/step - accuracy: 0.3794 - loss: 2.4195\nEpoch 1: val_accuracy improved from -inf to 0.83650, saving model to best_traffic_sign_model.h5\n\n\nWARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b981/981 ━━━━━━━━━━━━━━━━━━━━ 148s 136ms/step - accuracy: 0.3796 - loss: 2.4186 - val_accuracy: 0.8365 - val_loss: 0.5221 - learning_rate: 0.0010\nEpoch 2/50\n981/981 ━━━━━━━━━━━━━━━━━━━━ 0s 118ms/step - accuracy: 0.8532 - loss: 0.4538\nEpoch 2: val_accuracy improved from 0.83650 to 0.93687, saving model to best_traffic_sign_model.h5\n\n\nWARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b981/981 ━━━━━━━━━━━━━━━━━━━━ 121s 124ms/step - accuracy: 0.8532 - loss: 0.4537 - val_accuracy: 0.9369 - val_loss: 0.2297 - learning_rate: 0.0010\nEpoch 3/50\n981/981 ━━━━━━━━━━━━━━━━━━━━ 0s 120ms/step - accuracy: 0.9430 - loss: 0.1829\nEpoch 3: val_accuracy did not improve from 0.93687\n981/981 ━━━━━━━━━━━━━━━━━━━━ 142s 124ms/step - accuracy: 0.9430 - loss: 0.1829 - val_accuracy: 0.9361 - val_loss: 0.2087 - learning_rate: 0.0010\nEpoch 4/50\n981/981 ━━━━━━━━━━━━━━━━━━━━ 0s 117ms/step - accuracy: 0.9652 - loss: 0.1122\nEpoch 4: val_accuracy did not improve from 0.93687\n981/981 ━━━━━━━━━━━━━━━━━━━━ 119s 122ms/step - accuracy: 0.9652 - loss: 0.1122 - val_accuracy: 0.9202 - val_loss: 0.4381 - learning_rate: 0.0010\nEpoch 5/50\n619/981 ━━━━━━━━━━━━━━━━━━━━ 41s 115ms/step - accuracy: 0.9609 - loss: 0.1302\n\n\n\n# Plot training history\nplt.figure(figsize=(12, 4))\n\n\n# Plot training & validation accuracy\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='lower right')\n\n\n# Plot training & validation loss\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper right')\n\nplt.tight_layout()\nplt.show()\n\n\n# This is an optional advanced addition to help understand what the model is learning\nfrom tensorflow.keras.models import Model\nimport cv2\nimport numpy as np\n\n# Function to create a heatmap of what the CNN is focusing on\ndef generate_cam(model, img_array, class_idx, layer_name='conv2d_5'):\n    # Create a model that will output the last conv layer\n    grad_model = Model(\n        inputs=[model.inputs],\n        outputs=[model.get_layer(layer_name).output, model.output]\n    )\n\n    # Convert image to array and perform the same preprocessing\n    img_array = np.expand_dims(img_array, axis=0)\n\n    # Compute gradient of the predicted class with respect to feature map activations\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_array)\n        class_channel = predictions[:, class_idx]\n\n    # Gradient of the class output with respect to the output feature map\n    grads = tape.gradient(class_channel, conv_outputs)\n\n    # Vector of mean gradients for each channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # Weight feature maps by gradient importance\n    conv_outputs = conv_outputs[0]\n    heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n\n    # Normalize heatmap\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    heatmap = heatmap.numpy()\n\n    # Resize heatmap to original image size\n    heatmap = cv2.resize(heatmap, (img_array.shape[2], img_array.shape[1]))\n\n    # Convert to RGB for overlay\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n\n    # Superimpose heatmap on original image\n    superimposed_img = heatmap * 0.4 + img_array[0] * 255\n    superimposed_img = np.clip(superimposed_img, 0, 255).astype('uint8')\n\n    return superimposed_img\n\n\n# Additional imports for evaluation metrics\nimport os\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport seaborn as sns\n\n\n# Step 1: Load the mini holdout dataset\nmini_holdout_dir = '/content/'\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\nmini_holdout_generator = test_datagen.flow_from_directory(\n        mini_holdout_dir,\n        classes=['mini_holdout'],\n        target_size=image_size,\n        class_mode=None,\n        shuffle=False,\n        batch_size=1)\n\n\n# Step 2: Get the filenames from the generator\nfilenames = mini_holdout_generator.filenames\nbase_filenames = [os.path.basename(filename) for filename in filenames]\nprint(f\"Number of images in mini_holdout: {len(base_filenames)}\")\n\n\n# Step 3: Load ground truth labels from the CSV file\nground_truth_df = pd.read_csv('/content/mini_holdout_answers.csv')\nprint(\"First few rows of the ground truth data:\")\nprint(ground_truth_df.head())\nprint(f\"Number of entries in CSV: {len(ground_truth_df)}\")\n\n\n# Step 4: Generate predictions with our trained model\n# FIXED: Changed 'model' to 'improved_model'\nmini_holdout_generator.reset()\nmini_holdout_probabilities = improved_model.predict(mini_holdout_generator)\nmini_holdout_predictions = [np.argmax(probas) for probas in mini_holdout_probabilities]\n\n\n# Step 5: Create a DataFrame with predictions and filenames\npredictions_df = pd.DataFrame({\n    'Filename': base_filenames,\n    'Predicted_ClassId': mini_holdout_predictions\n})\n\n\n# Step 6: Merge with ground truth to ensure proper alignment\nevaluation_df = pd.merge(ground_truth_df, predictions_df, on='Filename', how='inner')\nprint(f\"Number of matched predictions: {len(evaluation_df)}\")\n\n\n# Step 7: Calculate accuracy\naccuracy = accuracy_score(evaluation_df['ClassId'], evaluation_df['Predicted_ClassId'])\nprint(f\"\\nModel Accuracy on Mini Holdout Set: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n\n\n# Step 8: Find the unique classes in the ground truth data\nunique_classes = sorted(evaluation_df['ClassId'].unique())\nprint(f\"\\nNumber of unique classes in mini holdout: {len(unique_classes)}\")\nprint(f\"Classes present: {unique_classes}\")\n\n\n# Create a mapping from class ID to index position for the classification report\npresent_target_names = [target_names[i] for i in unique_classes]\n\n\n# Step 9: Generate detailed classification metrics with only the present classes\nprint(\"\\nClassification Report:\")\nclass_report = classification_report(\n    evaluation_df['ClassId'],\n    evaluation_df['Predicted_ClassId'],\n    labels=unique_classes,  # Use only the classes that are present\n    target_names=present_target_names,\n    zero_division=0\n)\nprint(class_report)\n\n\n# Step 10: Create and visualize confusion matrix\nplt.figure(figsize=(12, 10))\ncm = confusion_matrix(\n    evaluation_df['ClassId'],\n    evaluation_df['Predicted_ClassId'],\n    labels=unique_classes  # Use only the classes that are present\n)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix for Mini Holdout Dataset')\nplt.tight_layout()\nplt.show()\n\n\n# Step 11: Visualize examples of correct and incorrect predictions\nplt.figure(figsize=(20, 16))\ncount_correct = 0\ncount_incorrect = 0\nexamples_to_show = 5\n\n\n# Create image mapping for easier lookup\nmini_holdout_generator.reset()\nimage_dict = {}\nfor i, image in enumerate(mini_holdout_generator):\n    if i &gt;= len(base_filenames):\n        break\n    image_dict[base_filenames[i]] = image[0]\n\n\n# Find examples of correct and incorrect predictions\nfor _, row in evaluation_df.iterrows():\n    is_correct = row['ClassId'] == row['Predicted_ClassId']\n\n    if ((is_correct and count_correct &lt; examples_to_show) or\n        (not is_correct and count_incorrect &lt; examples_to_show)):\n\n        image = image_dict.get(row['Filename'])\n        if image is None:\n            continue\n\n        if is_correct:\n            position = count_correct + 1\n            count_correct += 1\n            title_color = 'green'\n        else:\n            position = examples_to_show + count_incorrect + 1\n            count_incorrect += 1\n            title_color = 'red'\n\n        plt.subplot(2, examples_to_show, position)\n        plt.imshow((image * 255).astype(\"uint8\"))\n        true_name = target_names[row['ClassId']]\n        pred_name = target_names[row['Predicted_ClassId']]\n        plt.title(f\"True: {true_name}\\nPred: {pred_name}\", color=title_color)\n        plt.axis('off')\n        # Stop when we have enough examples of each\n    if count_correct &gt;= examples_to_show and count_incorrect &gt;= examples_to_show:\n        break\n\n\n\nplt.suptitle(\"Examples of Correct (top) and Incorrect (bottom) Predictions\", fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n\n\n# Step 12: Analyze which sign types have the highest error rates\nevaluation_df['is_correct'] = evaluation_df['ClassId'] == evaluation_df['Predicted_ClassId']\nerror_df = evaluation_df[~evaluation_df['is_correct']]\n\n\nif len(error_df) &gt; 0:\n    # Map class IDs to their names\n    error_df['true_sign_name'] = error_df['ClassId'].map(lambda x: target_names[x])\n\n    # Count errors by sign type\n    error_counts = error_df['true_sign_name'].value_counts()\n\n    print(\"\\nSign types with the highest error counts:\")\n    print(error_counts.head(10))\n\n    # Create a bar chart of the most frequently misclassified signs\n    plt.figure(figsize=(12, 6))\n    error_counts.head(min(10, len(error_counts))).plot(kind='bar')\n    plt.title('Sign Types with Highest Error Counts')\n    plt.xlabel('Sign Type')\n    plt.ylabel('Error Count')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    plt.show()\n\n\n # Step 13: Also analyze confusion patterns\n    if len(error_df) &gt; 5:  # Only if we have enough errors to analyze\n        error_df['pred_sign_name'] = error_df['Predicted_ClassId'].map(lambda x: target_names[x])\n        confusion_pairs = error_df.groupby(['true_sign_name', 'pred_sign_name']).size()\n        confusion_pairs = confusion_pairs.sort_values(ascending=False)\n\n        print(\"\\nMost common confusion patterns:\")\n        print(confusion_pairs.head(min(10, len(confusion_pairs))))\n        print(\"\\nThese patterns indicate which signs are most frequently confused with each other.\")\n\n\n# This code should be added after running the model and generating predictions\n\n# Create a detailed visualization of incorrectly classified images\ndef visualize_errors(evaluation_df, image_dict, target_names, max_examples=20):\n    \"\"\"\n    Creates a detailed visualization of incorrectly classified images\n\n    Parameters:\n    evaluation_df - DataFrame with true and predicted classes\n    image_dict - Dictionary mapping filenames to image data\n    target_names - List of class names\n    max_examples - Maximum number of examples to display\n    \"\"\"\n    # Filter for incorrect predictions only\n    error_df = evaluation_df[evaluation_df['ClassId'] != evaluation_df['Predicted_ClassId']].copy()\n\n    # Get the number of errors to display (up to max_examples)\n    num_errors = min(len(error_df), max_examples)\n\n    if num_errors == 0:\n        print(\"No errors found! The model correctly classified all images.\")\n        return\n\n    # Calculate grid dimensions\n    cols = 4  # 4 images per row\n    rows = (num_errors + cols - 1) // cols  # Ceiling division\n\n    # Create a larger figure to accommodate more images\n    plt.figure(figsize=(20, 5 * rows))\n\n    # Add each incorrect prediction to the plot\n    for i, (_, row) in enumerate(error_df.head(num_errors).iterrows()):\n        if i &gt;= max_examples:\n            break\n\n        # Get the image\n        image = image_dict.get(row['Filename'])\n        if image is None:\n            continue\n\n        # Create subplot\n        plt.subplot(rows, cols, i + 1)\n        plt.imshow((image * 255).astype(\"uint8\"))\n\n        # Get true and predicted class names\n        true_name = target_names[row['ClassId']]\n        pred_name = target_names[row['Predicted_ClassId']]\n\n        # Display detailed information\n        plt.title(f\"True: {true_name}\\nPred: {pred_name}\", color='red')\n        plt.xlabel(f\"Filename: {row['Filename']}\")\n        plt.axis('off')\n\n    plt.suptitle(f\"Incorrectly Classified Images (showing {num_errors} out of {len(error_df)} errors)\",\n                fontsize=16)\n    plt.tight_layout(rect=[0, 0, 1, 0.95])\n    plt.show()\n\n    # Additionally, group errors by true class for analysis\n    print(\"\\nErrors grouped by true sign type:\")\n    error_class_counts = error_df.groupby('ClassId').size()\n    for class_id, count in error_class_counts.items():\n        print(f\"{target_names[class_id]}: {count} errors\")\n\n    # Show confusion patterns (what signs get confused with what)\n    print(\"\\nDetailed confusion patterns:\")\n    error_df['true_sign_name'] = [target_names[x] for x in error_df['ClassId']]\n    error_df['pred_sign_name'] = [target_names[x] for x in error_df['Predicted_ClassId']]\n\n    confusion_patterns = error_df.groupby(['true_sign_name', 'pred_sign_name']).size()\n    for (true_name, pred_name), count in confusion_patterns.items():\n        print(f\"{true_name} mistaken as {pred_name}: {count} times\")\n\n# Call this function to visualize errors\nvisualize_errors(evaluation_df, image_dict, target_names, max_examples=20)\n\n\n\n\n Back to top"
  }
]